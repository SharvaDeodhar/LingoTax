{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LingoTax GNN Deduction Predictor — Training Notebook\n",
                "\n",
                "This notebook trains the GNN model on synthetic tax-profile data.\n",
                "Designed to run in **Google Colab** with GPU acceleration.\n",
                "\n",
                "**Author:** LingoTax Team (HackAI 2026)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 1: GPU Check"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1: Check GPU availability and print CUDA info\n",
                "import torch\n",
                "print(f'PyTorch version: {torch.__version__}')\n",
                "print(f'CUDA available:  {torch.cuda.is_available()}')\n",
                "if torch.cuda.is_available():\n",
                "    print(f'GPU device:      {torch.cuda.get_device_name(0)}')\n",
                "    print(f'CUDA version:    {torch.version.cuda}')\n",
                "else:\n",
                "    print('WARNING: No GPU detected. Training will run on CPU (slower).')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 2: Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2: Install dependencies (robust PyG install for Colab)\n",
                "import subprocess, sys, torch\n",
                "\n",
                "# Core deps\n",
                "subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q',\n",
                "    'pandas', 'numpy', 'scikit-learn', 'joblib', 'matplotlib'])\n",
                "\n",
                "# PyTorch Geometric — install matching your torch+CUDA version\n",
                "TORCH_VERSION = torch.__version__.split('+')[0]  # e.g. '2.1.0'\n",
                "CUDA_VERSION = torch.version.cuda or 'cpu'       # e.g. '12.1' or None\n",
                "CUDA_TAG = f'cu{CUDA_VERSION.replace(\".\", \"\")}' if CUDA_VERSION != 'cpu' else 'cpu'\n",
                "\n",
                "print(f'Installing PyG for torch={TORCH_VERSION}, cuda={CUDA_TAG}')\n",
                "try:\n",
                "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q',\n",
                "        'torch-geometric',\n",
                "        '-f', f'https://data.pyg.org/whl/torch-{TORCH_VERSION}+{CUDA_TAG}.html'])\n",
                "    print('PyTorch Geometric installed successfully!')\n",
                "except Exception as e:\n",
                "    print(f'PyG install failed ({e}). Falling back to MLP mode (no graph convolution).')\n",
                "    print('The model will still work — just without GraphSAGE layers.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 3: Clone Repository"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3: Clone the LingoTax repo\n",
                "# Replace <your-repo> with your actual GitHub URL.\n",
                "# If the repo is private, use a Personal Access Token (PAT):\n",
                "#   !git clone https://<PAT>@github.com/<org>/lingotax.git\n",
                "\n",
                "!git clone https://github.com/<your-org>/lingotax.git 2>/dev/null || echo 'Repo already cloned'\n",
                "%cd lingotax/model\n",
                "!ls -la"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 4: Generate Synthetic Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4: Generate 20,000 synthetic tax profiles\n",
                "!python data/gen_synthetic.py --n 20000 --distribution-aware --noise 0.05 --seed 42\n",
                "\n",
                "import pandas as pd\n",
                "df = pd.read_csv('data/users.csv')\n",
                "print(f'Generated {len(df)} profiles')\n",
                "print(df.head())\n",
                "print('\\nDeduction label sums:')\n",
                "for col in ['foreign_tax_credit', 'student_loan_interest', 'standard_deduction',\n",
                "            'earned_income_credit', 'child_tax_credit', 'educator_expense',\n",
                "            'ira_deduction', 'home_ownership_credit']:\n",
                "    print(f'  {col}: {df[col].sum()}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 5: Train the GNN Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5: Train the GNN (use --gpu if CUDA is available)\n",
                "import torch\n",
                "gpu_flag = '--gpu' if torch.cuda.is_available() else ''\n",
                "!python train/train_gnn.py --epochs 50 --hidden-dim 64 --lr 0.001 --seed 42 {gpu_flag}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 6: Visualize Results & Save to Google Drive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 6: View training metadata and save model to Google Drive\n",
                "import json\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "with open('models/metadata/gnn_v1.json') as f:\n",
                "    meta = json.load(f)\n",
                "\n",
                "print(f\"Macro AUC: {meta['metrics']['macro_auc']}\")\n",
                "print(f\"Training time: {meta['elapsed_seconds']}s\")\n",
                "print(f\"Train size: {meta['train_size']}\")\n",
                "\n",
                "# Per-deduction AUC bar chart\n",
                "deductions = [k for k in meta['metrics'] if k != 'macro_auc']\n",
                "aucs = [meta['metrics'][d]['auc'] for d in deductions]\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.barh(deductions, aucs, color='#4F46E5')\n",
                "plt.xlabel('ROC AUC')\n",
                "plt.title('GNN Deduction Predictor — Per-Deduction AUC')\n",
                "plt.xlim(0, 1)\n",
                "plt.tight_layout()\n",
                "plt.savefig('models/metadata/auc_chart.png', dpi=150)\n",
                "plt.show()\n",
                "\n",
                "# Save to Google Drive\n",
                "try:\n",
                "    from google.colab import drive\n",
                "    drive.mount('/content/drive')\n",
                "    import shutil\n",
                "    dst = '/content/drive/MyDrive/lingotax_models/'\n",
                "    !mkdir -p {dst}\n",
                "    shutil.copy('models/checkpoints/gnn_v1.pt', dst)\n",
                "    shutil.copy('models/checkpoints/gnn_v1.meta.json', dst)\n",
                "    shutil.copy('models/metadata/gnn_v1.json', dst)\n",
                "    print(f'Model saved to Google Drive: {dst}')\n",
                "except Exception as e:\n",
                "    print(f'Google Drive save skipped: {e}')\n",
                "    print('Model is available at: models/checkpoints/gnn_v1.pt')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 7 (Optional): Test Inference via FastAPI"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 7 (Optional): Launch FastAPI in background and test with curl\n",
                "# Uncomment to run:\n",
                "# import subprocess\n",
                "# proc = subprocess.Popen(['uvicorn', 'api.fastapi_infer:app', '--host', '0.0.0.0', '--port', '8001'])\n",
                "# import time; time.sleep(3)\n",
                "# !curl -X POST http://localhost:8001/predict_deductions \\\n",
                "#   -H 'Content-Type: application/json' \\\n",
                "#   -d '{\"visa_type\": \"H-1B\", \"filing_status\": \"single\", \"total_income\": 75000, \"foreign_income\": 10000, \"foreign_tax_paid\": 1, \"state\": \"OH\", \"num_dependents\": 0, \"years_in_us\": 4}'\n",
                "# proc.terminate()\n",
                "print('Uncomment the lines above to test the FastAPI inference endpoint.')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        },
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "accelerator": "GPU"
    },
    "nbformat": 4,
    "nbformat_minor": 4
}