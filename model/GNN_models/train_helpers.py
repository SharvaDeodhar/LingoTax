"""
Training helpers — data loading, feature encoding, dataset construction.

Purpose:
    Load CSV generated by gen_synthetic.py, encode categorical features,
    build train/val splits, and return ready-to-use tensors.

Author: LingoTax Team (HackAI 2026)
"""

import logging
import os
from pathlib import Path
from typing import Tuple

import joblib
import numpy as np
import pandas as pd
import torch
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler

logger = logging.getLogger(__name__)

# ── Constants ─────────────────────────────────────────────────────────────────

DEDUCTIONS = [
    "foreign_tax_credit",
    "student_loan_interest",
    "standard_deduction",
    "earned_income_credit",
    "child_tax_credit",
    "educator_expense",
    "ira_deduction",
    "home_ownership_credit",
]

CATEGORICAL_COLS = ["visa_type", "filing_status", "state"]

NUMERICAL_COLS = [
    "num_dependents",
    "total_income",
    "foreign_income",
    "foreign_tax_paid",
    "student_loan_interest_paid",
    "paid_tuition",
    "owns_home",
    "years_in_us",
]

ENCODERS_DIR = Path(__file__).parent / "encoders"


def load_csv(csv_path: str | None = None) -> pd.DataFrame:
    """Load the synthetic users CSV."""
    if csv_path is None:
        csv_path = Path(__file__).parent.parent / "data" / "users.csv"
    df = pd.read_csv(csv_path)
    logger.info("Loaded %d rows from %s", len(df), csv_path)
    return df


def fit_encoders(df: pd.DataFrame, save: bool = True) -> Tuple[OneHotEncoder, StandardScaler]:
    """Fit OneHotEncoder and StandardScaler on the dataframe, optionally save."""
    ohe = OneHotEncoder(sparse_output=False, handle_unknown="ignore")
    ohe.fit(df[CATEGORICAL_COLS])

    scaler = StandardScaler()
    scaler.fit(df[NUMERICAL_COLS])

    if save:
        ENCODERS_DIR.mkdir(parents=True, exist_ok=True)
        joblib.dump(ohe, ENCODERS_DIR / "ohe.joblib")
        joblib.dump(scaler, ENCODERS_DIR / "scaler.joblib")
        logger.info("Saved encoders to %s", ENCODERS_DIR)

    return ohe, scaler


def load_encoders() -> Tuple[OneHotEncoder, StandardScaler]:
    """Load previously saved encoders."""
    ohe = joblib.load(ENCODERS_DIR / "ohe.joblib")
    scaler = joblib.load(ENCODERS_DIR / "scaler.joblib")
    return ohe, scaler


def encode_features(
    df: pd.DataFrame,
    ohe: OneHotEncoder,
    scaler: StandardScaler,
) -> np.ndarray:
    """Transform a dataframe into a numeric feature matrix."""
    cat_encoded = ohe.transform(df[CATEGORICAL_COLS])
    num_scaled = scaler.transform(df[NUMERICAL_COLS])
    features = np.hstack([cat_encoded, num_scaled])
    return features.astype(np.float32)


def build_dataset(
    seed: int = 42,
    small: bool = False,
    csv_path: str | None = None,
    val_fraction: float = 0.15,
) -> dict:
    """
    Build complete dataset dictionary with train/val splits.

    Returns
    -------
    dict with keys:
        X_train, X_val    — torch.Tensor (N, feat_dim)
        y_train, y_val    — torch.Tensor (N, num_deductions)
        input_dim         — int
        deduction_names   — list[str]
    """
    df = load_csv(csv_path)

    if small:
        df = df.head(500)
        logger.info("--small mode: using first 500 rows")

    ohe, scaler = fit_encoders(df, save=True)
    X = encode_features(df, ohe, scaler)
    y = df[DEDUCTIONS].values.astype(np.float32)

    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=val_fraction, random_state=seed,
    )

    return {
        "X_train": torch.tensor(X_train),
        "X_val": torch.tensor(X_val),
        "y_train": torch.tensor(y_train),
        "y_val": torch.tensor(y_val),
        "input_dim": X_train.shape[1],
        "deduction_names": DEDUCTIONS,
    }
